[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring polular Spotify Tracks",
    "section": "",
    "text": "What makes a song popular in todays world? Is there a secret receipt for success? In this project I investigate these and further questions using a open source data set of Spotify tracks.\nAbout the data set: The dataset is collated from Spotify’s API using two separate python scripts to extract popular and non-popular songs and their associated audio and descriptive features. Descriptive features of a song include information about the song such as the artist name, album name and release date. Audio features include key, valence, danceability and energy which are results of spotify’s audio analysis.\nStep 1. Load the libraries\n#renv::install(\"ISLR\")\nlibrary(tidyverse) #basic\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tibble) #tibble\nlibrary(ggplot2) #ggplot\nlibrary(ggExtra) #for marginal histograms and more\nlibrary(ggrepel) #label for graphs\nlibrary(ggcorrplot) # for ggpcorrpltos\nlibrary(ggthemes)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nlibrary(viridisLite)\nlibrary(scales) #palettes for high readability and accessibility for visualisations\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:viridis':\n\n    viridis_pal\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(grid)\nlibrary(gt) # for tables\nlibrary(gtsummary)\nlibrary(ggsurvey)\n\nLoading required package: survey\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoading required package: survival\n\nAttaching package: 'survey'\n\nThe following object is masked from 'package:graphics':\n\n    dotchart\n\nLoading required package: hexbin\n\nlibrary(gtExtras) # for tables\nlibrary(webshot2) # save gt tables as png\nlibrary(data.table) #transpose data frames\n\n\nAttaching package: 'data.table'\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(extrafont) # for extra fonts\n\nRegistering fonts with R\n\nlibrary(lubridate) #dates to numeric\nlibrary(showtext) #for adding fonts\n\nLoading required package: sysfonts\nLoading required package: showtextdb\n\nAttaching package: 'showtextdb'\n\nThe following object is masked from 'package:extrafont':\n\n    font_install\n\nlibrary(caret) #train test data set\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:survival':\n\n    cluster\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(glmnet) # for ridge regression\n\nLoaded glmnet 4.1-8\n\nlibrary(ISLR) # for splines and polys\nlibrary(splines)\nloadfonts(device=\"win\", quiet= TRUE)\nfont_add_google(name=\"Barlow\", family=\"Barlow\")\nshowtext_auto()\noptions(\"scipen\"=100, \"digits\"=4)\nStep 2. Global options\noptions(digits=3)\nStep 3. Import data\ndata &lt;- read.csv(\"data/high_popularity_spotify_data.csv\")\nStep 4. Check data\nprint(\"Are there any missing values in the data set?\")\n\n[1] \"Are there any missing values in the data set?\"\n\nsum(is.na(data))\n\n[1] 1\n\nprint(\"only 1 missing value. Exclude it\")\n\n[1] \"only 1 missing value. Exclude it\"\n\ndata &lt;- data %&gt;% na.omit(data)\nprint(\"Is the data in a tibble format?\")\n\n[1] \"Is the data in a tibble format?\"\n\nis_tibble(data)\n\n[1] FALSE\nprint(\"convert into a tibble format\")\nas_tibble(data)\n#explore the structure\nstr(data)\nStep 5. Create an own theme\ntheme_own &lt;- function(){\n  theme_minimal() +\n    theme(text=element_text(size=12,family = \"Barlow\"),\n          axis.text.x = element_text(size = 10, angle=90),\n          axis.text.y = element_text(size = 10),\n          axis.title = element_text(size = 11, face = \"bold\"),\n          panel.grid.minor = element_blank(),\n          plot.title = element_text(size=14, face=\"bold\"),\n          panel.grid.major = element_line(colour = \"grey70\", linewidth =  0.2),\n          legend.position=\"bottom\",\n          legend.title = element_text(size=11),\n          legend.text = element_text(size=10),\n          strip.text = element_text(size = 10),\n          strip.placement = \"outside\",\n          panel.spacing = unit(1, \"lines\"),\n          strip.background = element_blank()) }\nshow_col(viridis_pal(option = \"inferno\")(20))\n\n\n\n\n\n\n\n#1 Farbe\nvi.col_1 &lt;- \"#5D126EFF\" \nvi.col_1.2 &lt;- \"#FCA50AFF\" \nvi.col_1.3 &lt;- \"#F8850FFF\"\nvi.col_1.4 &lt;- \"#DD513AFF\"\n#2 Farben \nvi.col_2 &lt;- c(\"#5D126EFF\", \n              \"#FCA50AFF\") \n\nvi.col_light &lt;- c(\"#FCFFA4FF\",\n                  \"#FAC62DFF\",\n                  \"#FCA50AFF\",\n                  \"#F17020FF\", \n                  \"#DD513AFF\",\n                  \"#B1325AFF\",\n                  \"#932667FF\")\n# 6 Farben\nvi.col_6 &lt;- c(\"#0C0826FF\",\n              \"#5D126EFF\", \n              \"#932667FF\", \n              \"#DD513AFF\", \n              \"#FCA50AFF\", \n              \"#FAC62DFF\"  \n              )"
  },
  {
    "objectID": "index.html#how-are-danceability-and-energy-are-connected",
    "href": "index.html#how-are-danceability-and-energy-are-connected",
    "title": "Exploring polular Spotify Tracks",
    "section": "How are danceability and energy are connected?",
    "text": "How are danceability and energy are connected?\nCreate a density plot to explore, how the distributions overlap. Use all data for it.\n\nenergy_dance_dense_plot &lt;- ggplot(data, aes(x=danceability, y= energy))+\n  geom_point(colour=\"#1F988BFF\")+\n  geom_density_2d_filled(alpha= 0.5)+\n  geom_density_2d(colour=\"#440154FF\")+\n  labs(title=\"Energy and Danceability\", subtilte=\"Density Plot\")+\n  scale_color_viridis_c(option=\"inferno\")+\n  scale_y_continuous(breaks=seq(0,1,0.25), limits=c(0,1))+\n  scale_x_continuous(breaks=seq(0,1,0.25),limits=c(0,1))+\n  theme_own()\nenergy_dance_dense_plot  \n\n\n\n\n\n\n\n\nKey insights:\n\nYou can see from this graph, that tracks on spotify are high in energy and danceability (the density under 0.25 in energy and danceability are very low, the highest density is around 0.75 and quite high) → our inituition would say us, that a lot of songs are characterized by high energy and high density. The density plot gives a first hint, that this could actually the case.\n\nLet’s create a scatter plot to investigates connections. It gives good insgiths in possible relationsships. Use the data set with only the 6 biggest genres in it, so the plot isn’t to messy\n\nenergy_dance_scat_plot &lt;- ggplot(data, aes(danceability, energy))+\n  geom_point(alpha=0.7,aes(color= genre100))+\n  labs(title= \"Energy and dancability by genre\", subtitle=\"Scatter plot\", color= \"Playlist genre\")+\n  scale_color_viridis(discrete=TRUE, option=\"inferno\")+\n  scale_y_continuous(breaks=seq(0,1,0.25), limits=c(0,1))+\n  scale_x_continuous(breaks=seq(0,1,0.25),limits=c(0,1))+\n  theme_own()\nenergy_dance_scat_plot\n\n\n\n\n\n\n\n\nKey insights:\n\nwhereas rock and pop music tracks could also have lower danceability, the most tracks of the other genres are characterized by an higher dancability than 0.4\ninteresting is, that rock music has higher energy, but if the energy level is lower, they can have also I higher dancability.\nmost of the electronic tracks are high in energy.\nespecially for energy you can see that the vast majority is high in energy\n\nLet’s plot the density in direct comparison, neglecting the genres.\n\nenergy_dance_dense_plot &lt;- ggplot(data)+\n  geom_density(aes(energy, ..density..), fill=vi.col_1)+ #top\n  geom_label(aes(x=0.9, y=0.8,\n                 label=\"Energy\"), color=vi.col_1)+\n  geom_density(aes(danceability, -..density..), fill=vi.col_1.2)+ #bottom\n  geom_label(aes(x=0.9, y=-0.8,\n                 label=\"Danceability\"), color=vi.col_1.2)+\n  theme_own()+\n  labs(title= \"Comparision of Danceability and Energy Distributions\", subtitle= \"Kernel Density plots\", xlab=\"Score of Energy / Danceability\")\nenergy_dance_dense_plot\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning in geom_label(aes(x = 0.9, y = 0.8, label = \"Energy\"), color = vi.col_1): All aesthetics have length 1, but the data has 1685 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_label(aes(x = 0.9, y = -0.8, label = \"Danceability\"), color = vi.col_1.2): All aesthetics have length 1, but the data has 1685 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nThat looks so connected ! Most of the tracks in the data set has a very high danceability and a higher energy!!\nShall we compare the distributions of these items more detailed using kernel densitiy distributions and compare the genres?\n\nenergy_dense_plot &lt;- ggplot(data, aes(energy, ..scaled..))+\n  geom_density(alpha=0.4, col= vi.col_1, fill=vi.col_1)+\n  labs(title= \"Energy score distributions by genres\", subtitle = \"Kernel Densitity plot\", x= \"Energy\", y= \"Density\", )+\n  theme_own()+\n  facet_wrap(~genre100)\nenergy_dense_plot\n\n\n\n\n\n\n\n\n\ndance_dense_plot &lt;- ggplot(data, aes(danceability, ..scaled..))+\n  geom_density(alpha=0.4, col= vi.col_1.3, fill=vi.col_1.3)+\n  labs(title= \"Danceability score distributions by genres\", subtitle = \"Kernel Densitity plot\", x= \"Danceability\", y= \"Density\", )+\n  theme_own()+\n  facet_wrap(~genre100)\ndance_dense_plot\n\n\n\n\n\n\n\n\nKey insights:\n\nWhile electronic, pop, and rock are very high in energy, especially latin and hip hop music has a very high danceability\nit seems like that songs, that have a middle to high danceability has a middle to high energy.\n\nIt seems like danceability and energy are somehow connected. Are there songs, that have a higher danceability and are lower in energy or vice versa? Let’s find out using the whole data set this time!\n\nhighenergy_lowdance &lt;- data %&gt;% filter(energy&gt; 0.25 & danceability&lt; 0.25) %&gt;% dplyr::select(track_artist, track_name, playlist_genre, track_popularity, energy, tempo, danceability, loudness, liveness, valence, speechiness,instrumentalness,acousticness,playlist_subgenre)\nhighenergy_lowdance\n\n         track_artist                           track_name playlist_genre\n1 Stone Temple Pilots Interstate Love Song - 2019 Remaster           rock\n2            Coldplay                              Fix You            pop\n3 My Chemical Romance             I'm Not Okay (I Promise)           punk\n4 My Chemical Romance                     The Ghost of You           punk\n5      Lynyrd Skynyrd                            Free Bird           rock\n  track_popularity energy tempo danceability loudness liveness valence\n1               72  0.927   171        0.215    -7.33   0.1260   0.490\n2               82  0.417   138        0.209    -8.74   0.1130   0.124\n3               74  0.940   180        0.210    -3.43   0.2690   0.255\n4               70  0.886   146        0.202    -3.81   0.6430   0.192\n5               75  0.834   118        0.249    -8.21   0.0924   0.337\n  speechiness instrumentalness acousticness playlist_subgenre\n1      0.0452        0.0066100     0.000273           classic\n2      0.0338        0.0019600     0.164000              soft\n3      0.1230        0.0000000     0.006020          pop punk\n4      0.0816        0.0000000     0.028600          pop punk\n5      0.0574        0.0000969     0.074200          southern\n\n\nLooks like tracks with high energy and low danceability are kind of outliers and deviates from the patterns. If you inspect the table you can see, that all the five songs have low speechiness, low instrumentalness and low acoustics in common. Though, they don’t belong to the same genre.\nHaving a look on the next table, where tracks have a high danceability and low energy, you can see this is a much more common combination. Especially ambient tracks have these characteristics.\n\nlowenergy_highdance &lt;- data %&gt;% filter(energy&lt; 0.25 & danceability&gt; 0.25) %&gt;% dplyr::select(track_artist, track_name, playlist_genre, track_popularity, energy, tempo, danceability, loudness, liveness, valence, speechiness,instrumentalness,acousticness,playlist_subgenre) %&gt;% arrange(danceability)\nprint(\"This are the top ten at the list, ordered by danceability\")\n\n[1] \"This are the top ten at the list, ordered by danceability\"\n\nhead(lowenergy_highdance,10)\n\n                              track_artist                 track_name\n1                                 leadwave                   memories\n2  The Cinematic Orchestra, Patrick Watson            To Build A Home\n3                           Gibran Alcocer                     Idea 1\n4                       Fabrizio Paterlini                      Waltz\n5                              Hans Zimmer                       Time\n6                           Patrick Watson   Je te laisserai des mots\n7                            Billie Eilish         listen before i go\n8                          Whitney Houston     I Will Always Love You\n9                  Lana Del Rey, Bleachers Margaret (feat. Bleachers)\n10                          Gibran Alcocer                     Idea 9\n   playlist_genre track_popularity energy tempo danceability loudness liveness\n1      electronic               70 0.0823  67.2        0.252    -27.3   0.0857\n2            folk               72 0.1220 148.7        0.264    -15.4   0.0940\n3       classical               71 0.1460 172.1        0.265    -21.1   0.1040\n4       classical               71 0.0516  87.6        0.270    -33.8   0.1120\n5       classical               71 0.0968  62.9        0.286    -16.8   0.0861\n6            folk               83 0.1870 132.7        0.303    -16.8   0.1020\n7         ambient               74 0.0561  79.8        0.319    -23.0   0.3880\n8           blues               75 0.2140  67.5        0.332    -12.5   0.0839\n9         ambient               78 0.2450 103.8        0.340    -13.4   0.0633\n10      classical               72 0.1380 128.6        0.340    -24.9   0.1120\n   valence speechiness instrumentalness acousticness playlist_subgenre\n1   0.0381      0.0344       0.92800000        0.951         vaporwave\n2   0.0735      0.0349       0.34900000        0.885             indie\n3   0.4160      0.0323       0.89300000        0.985     neo-classical\n4   0.1990      0.0442       0.95300000        0.986     neo-classical\n5   0.0395      0.0323       0.69600000        0.178     neo-classical\n6   0.2120      0.0356       0.49900000        0.989             indie\n7   0.0820      0.0450       0.00384000        0.935          academic\n8   0.1100      0.0349       0.00000562        0.845           classic\n9   0.1920      0.0319       0.09920000        0.969          academic\n10  0.3250      0.0357       0.84600000        0.984     neo-classical\n\nprint(\"How many tracks meet this characteristics?\")\n\n[1] \"How many tracks meet this characteristics?\"\n\nnrow(lowenergy_highdance)\n\n[1] 41"
  },
  {
    "objectID": "index.html#which-role-plays-the-tempo-for-danceability-and-energy",
    "href": "index.html#which-role-plays-the-tempo-for-danceability-and-energy",
    "title": "Exploring polular Spotify Tracks",
    "section": "Which role plays the tempo for danceability and energy?",
    "text": "Which role plays the tempo for danceability and energy?\nFirst of all, tempo is often seen as a good indicator for genres. Let’s explore if this is true for this data with a dot plot!\n\n# Prepare the data and get the mean and standard deviation\ntempo_dot_data &lt;- data %&gt;% group_by(playlist_genre) %&gt;% dplyr::summarise(Mean= mean(tempo), SD= sd(tempo))\n\n# Plot a cleaveland dot plot\ntempo_genre_dot &lt;- ggplot(tempo_dot_data, aes(x=reorder(playlist_genre, Mean), y=Mean))+\n                          geom_pointrange(aes(ymin = Mean - SD, ymax = Mean + SD ), size=1, col=vi.col_1)+\n   labs(title= \"The average tempo and the standard deviation per genre\", subtitle=\"Cleaveland Dotplot\", x= \"Genre\", y=\"Tempo in BPM\")+\n                            theme_own()+\n                            coord_flip()\ntempo_genre_dot\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\nOverall, the average BPM in each genre is quite high! That is a bit surprising, because music experts detected lower BPM are common for certain music types. For instance, Hip Hop is normally characterized by 85 to 95 BPM. Could be a hint, that popular tracks are a bit faster, than usual for their genre.\nLet’s create a categorial variable for tempo to explore, how tempo is connected to danceability and energy.\nFirst, plot a density plot to see the distribution for useful categorization. Use the full data set this time.\n\ntempo_dense_plot &lt;- ggplot(data, aes(tempo))+\n  geom_density(alpha=0.6,color=vi.col_1, fill=vi.col_1)+\n  geom_histogram(aes(y=..density..), colour=vi.col_1,fill= vi.col_1, alpha=0.2)+\n    scale_x_continuous(breaks=seq(0,250,50), limits=c(0,250))+\n  labs(title= \"Tempo distribution of tracks\", subtitle = \"Densitity plot\", x= \"Density\", y= \"Tempo in BPM\")+\n  theme_own()\ntempo_dense_plot\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\nYou can see, there is a wide range of tempo! Let’s build categories out of it!\n\ndata&lt;- data %&gt;% mutate(tempo_cat = case_when(tempo &lt; 90 ~ \"less than 90\",\n                                                    tempo &lt;= 120 ~ \"up to 120\",\n                                                    tempo &lt;= 150 ~ \"up to 150\",\n                                                  tempo &lt;=  180 ~ \"up to 180\",\n                                                  tempo &gt;  180 ~ \"more than 180\"))\n#relevel\ndata$tempo_cat &lt;- factor(data$tempo_cat, levels= c(\"less than 90\",\"up to 120\",\"up to 150\",\"up to 180\", \"more than 180\"))\n\n\nenergy_dance_tempo_plot &lt;- ggplot(data, aes(danceability, energy))+\n  geom_point(alpha=0.7, col=vi.col_1.3)+\n  labs(title= \"Energy and dancability by genre\", subtitle=\"Scatter plot\", color= \"Playlist genre\")+\n  scale_color_viridis(discrete=TRUE, option=\"inferno\")+\n  scale_y_continuous(breaks=seq(0,1,0.25), limits=c(0,1))+\n  scale_x_continuous(breaks=seq(0,1,0.25),limits=c(0,1))+\n  theme_own()+\n  facet_wrap(~tempo_cat)\nenergy_dance_tempo_plot\n\n\n\n\n\n\n\n\nLooks like over 180 BPM is a bit hard to dance to. Though the energy is very high!\nOverall, the established range of songs between 90 and 120 BPM has in general the best scores for danceability and energy. Not that much of surprise, isn’t it?\nSo far, we investigated how three of the features are related to each other using data visualization. Following the former steps of data visualitzation we are only able to plot three variables at once. Of course, we could add a forth variable to it, using another group variable in the “Energy and dance by genre” graph. But in doing this, the graph isn’t very concise anymore and the recipient will need a lot of time to find out what is exactly going on. So, let’s move to the next step for plotting multiple relationships of the features at once."
  },
  {
    "objectID": "index.html#how-do-each-of-the-features-correlate-with-each-other",
    "href": "index.html#how-do-each-of-the-features-correlate-with-each-other",
    "title": "Exploring polular Spotify Tracks",
    "section": "How do each of the features correlate with each other?",
    "text": "How do each of the features correlate with each other?\nWith a correlation plot, we can visualize the relationships of all features at once.\n\n# subset the data set to extract only features out of it\nfeatures_data &lt;- data %&gt;% dplyr::select(energy, tempo, danceability, loudness, liveness, valence, speechiness,  instrumentalness,acousticness,duration_ms, key, mode)\n# create a correlation matrix\nfeatures_cor &lt;- round(cor(features_data),1)\n# create a corrplot\nfeatures_corrplot &lt;- ggcorrplot(features_cor, \n                                hc.order=TRUE, #use hierarchical clustering for better overview\n                                type= \"upper\", #only the upper part for better overview, cause two sided correlation\n                                outline.col = \"white\",\n                                lab=TRUE) + #get labels\n  scale_fill_gradientn(colors = vi.col_light)\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\nfeatures_corrplot\n\n\n\n\n\n\n\n\nWhat we can see from the plot is that only vert few features are connected to each other, interesting!\nNot connected with any feature in a remarkable way:\n- key, speechiness, liveness, mode, tempo, duration and instrumentalness have no or only a minimum (&gt;=0.2) correlation to the other features - valence and danceability and valence and energy has a small correlation of 0.3 -&gt; there is a small correlation in the positiveness of a track and their danceability and energy, what seems to make sense, don’t you think? - energy and loudness (0.6) have a positive, moderate correlation -&gt; the more loud the music is, the more energy is in the music and vice versa - also, the energetic the music is, the less acoustic a track is (-0.7) (and vice versa) - the more acoustic (-.05) or instrumental (-0.4) a track is, the less loud is song. - was is especially interesting is, that the correlation of energy and danceability is zero, which means, no linear connection can’t be found. This is very contradictory what can be assessed from the data visualisation before. That does not necessarily mean that there isn’t any realtionship at all, it is more an indicator, that no linear relationship can’t be found in the data."
  },
  {
    "objectID": "index.html#popularity-by-genre",
    "href": "index.html#popularity-by-genre",
    "title": "Exploring polular Spotify Tracks",
    "section": "Popularity by genre",
    "text": "Popularity by genre\nNow, let’s explore the popularity by genre.\n\npop_data &lt;- data %&gt;% group_by(playlist_genre) %&gt;% summarize(Mean=mean(track_popularity), SD=sd(track_popularity))\npop_dot&lt;- ggplot(pop_data, aes(y= Mean, x=reorder(playlist_genre, Mean)), color=vi.col_1)+\n  geom_pointrange(aes(ymin = Mean - SD, ymax = Mean + SD ), color=vi.col_1)+\n  labs(title=\"Popularity by Genre\", subtitle=\"Cleaveland Dotplot\")+\n  scale_y_continuous(limits=c(60,90))+\n  theme_own()+\n  labs(y=\"Genre\", x=\"Popularity\")+\n  coord_flip()\npop_dot\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\n\nGaming and pop are by far the most genres with the most popular songs on overage, while jazz, korean, indian, classical and reggae genres contains the least popular tracks."
  },
  {
    "objectID": "index.html#is-very-happy-or-very-sad-or-angry-music-more-popular",
    "href": "index.html#is-very-happy-or-very-sad-or-angry-music-more-popular",
    "title": "Exploring polular Spotify Tracks",
    "section": "Is very happy or very sad or angry music more popular?",
    "text": "Is very happy or very sad or angry music more popular?\nThe valence indicates the overall musical positiveness(emotion) of a track. High valence sounds happy; low valence sounds sad or angry. How are the valence and the track’s popularity connected? Is very sad, angry or happt music more popular than emotional neutral music?\n\nval_pop_scat_plot &lt;- ggplot(data, aes(valence, track_popularity))+\n  geom_point(alpha=0.7,col=vi.col_1)+\n  geom_smooth(method=loess, se=TRUE, color=vi.col_1.3)+\n  labs(title= \"Musical positiveness and popularity by genre\", subtitle=\"Scatter plot with linear polynomial regression\", color= \"Playlist genre\", x=\"Musical positiveness\", y=\"Popularity\")+\n  scale_color_viridis(discrete=TRUE, option=\"inferno\")+\n    scale_x_continuous(breaks=seq(0,1,0.25),limits=c(0,1))+\n  theme_own()\nval_pop_scat_plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNo relation visible.\nOf course, we could now move on to the other futures and visualize the relationships. A better method is, to find a suitable model, where all impactful features are included and weighted according their relevance for the tracks popularity. For this, let’s create a training set and a test and validation set. Precondition is, that we have each track only once in the data set, so we must remove duplicates from the set.\n\n#get songs in minutes instead of miliseconds, because could construct some bias\nfeatures_data &lt;- data %&gt;% mutate(duration_min = duration_ms/60000)\n# remove the duplicates \nfeatures_data &lt;- features_data %&gt;% distinct(track_name, .keep_all = TRUE)\n# only features and predictor\nfeatures_data &lt;- features_data %&gt;% dplyr::select(track_popularity,  energy,tempo, danceability,liveness, loudness,valence, speechiness, instrumentalness, mode, key, acousticness, duration_min)\n\n#create partition\ntrain_index &lt;- createDataPartition(features_data$track_popularity, p = .7, \n                                  list = FALSE, \n                                  times = 1)\n#create training data\ntrain_data &lt;- features_data[train_index,]\n\n#create val and test set\nval_test_data &lt;- features_data[-train_index,]\n\n# remaining 30% into validation and test set\nval_index &lt;- createDataPartition(val_test_data$track_popularity, p = .6, \n                                  list = FALSE, \n                                  times = 1)\nval_data &lt;- val_test_data[val_index,]\ntest_data &lt;- val_test_data[-val_index,]\n\n# Outcome of this section is that the data (100%) is split into:\n# training (~70%)\n# validation (~20%)\n# test (~10%)\n\nAre the popularity equally distributed in the training, validation and test set?\n\nhist &lt;- ggplot()+\n  geom_density(data= train_data, aes(x=track_popularity, color= \"t\"), size=1, alpha=0.2)+ \n  geom_density(data= val_data, aes(x=track_popularity, color=\"v\"), size=1)+\n  geom_density(data= test_data, aes(x=track_popularity, color=\"te\"), size=1)+\n  scale_color_manual(name= \"color\", values=c(\"t\"=vi.col_1, \"v\"=vi.col_1.2,\"te\"=vi.col_1.4), labels=c(\"test\", \"valid\", \"train\"))+\n  theme_minimal()\n\nhist\n\n\n\n\n\n\n\n\nWhile the test and train set looks very similar, the validation set is a bit volatil and has less unpopular tracks."
  },
  {
    "objectID": "index.html#a-simple-linear-regression-model",
    "href": "index.html#a-simple-linear-regression-model",
    "title": "Exploring polular Spotify Tracks",
    "section": "A simple linear regression model",
    "text": "A simple linear regression model\nAll in all, we have 12 features, with which the tracks can be characterized. If we compute a linear regression model, we can determine the average effect of a feature, if the feature increases by 1, holding the other predictors fixed. Thus, it is necessary for the interpretation to keep the feature’s bandwith in mind. Let’s create some descriptives of the features and the outcome.\n\n#do we have missing values in here?\ntable(is.na(features_data))\n\n\nFALSE \n18291 \n\n# let's get the summaries\nfeatures_descr&lt;- features_data %&gt;% dplyr::summarise_all(list(min=min, max=max, avg=mean, SD=sd)) \n  #get a nicer data frame\nfeatures_descr &lt;- features_descr %&gt;% pivot_longer(cols=everything(), names_to = c(\"item\", \".value\"), names_pattern=\"(.*)_(.*)\") %&gt;% as_tibble() \nfeatures_descr &lt;- features_descr %&gt;%mutate(across(where(is.numeric), round,2))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n  # get a nice table\nfeatures_data_table &lt;- features_descr %&gt;% gt() %&gt;%\n  tab_header(title=\"Descriptives of features\",subtitle=\"Minimum, Maximum, Average and SD of the features\") %&gt;%\n  cols_label(\n    item= \"Feature\",\n    min = \"Minimum\",\n    max = \"Maximum\") %&gt;%\n  fmt_number(rows=c(1,2),\n             decimals = 0) %&gt;%\n  tab_options(column_labels.background.color = vi.col_1)%&gt;%\n  tab_style(\n    style= cell_text(\n      weight= 'bold',\n      font = google_font(\"Barlow\")),\n    locations=cells_title(groups='title')) %&gt;%\n    tab_style(\n    style= list(cell_fill(color= \"thistle\"),\n                cell_text(weight=\"bold\")),\n    locations=cells_body(columns=item)) %&gt;%\n  gt_theme_nytimes()\nfeatures_data_table\n\n\n\n\n\n\n\nDescriptives of features\n\n\nMinimum, Maximum, Average and SD of the features\n\n\nFeature\nMinimum\nMaximum\navg\nSD\n\n\n\n\ntrack_popularity\n68\n100\n75\n6\n\n\nenergy\n0\n1\n1\n0\n\n\ntempo\n49.30\n209.69\n121.27\n27.33\n\n\ndanceability\n0.14\n0.98\n0.65\n0.16\n\n\nliveness\n0.02\n0.95\n0.17\n0.13\n\n\nloudness\n-43.64\n1.29\n-6.83\n3.55\n\n\nvalence\n0.03\n0.98\n0.53\n0.24\n\n\nspeechiness\n0.02\n0.85\n0.10\n0.10\n\n\ninstrumentalness\n0.00\n0.97\n0.04\n0.16\n\n\nmode\n0.00\n1.00\n0.58\n0.49\n\n\nkey\n0.00\n11.00\n5.33\n3.61\n\n\nacousticness\n0.00\n1.00\n0.23\n0.26\n\n\nduration_min\n1.03\n9.12\n3.57\n0.98\n\n\n\n\n\n\n\nWe can clearly see, that the scores have very different ranges. One option is, to normalize the scores, so each score has an average of zero, while the highest possible score is 1 and the lowest possible score is 1. This has the big disadvantage, that it is not that straighforward to interprete the results with such scores. With normalized scores, one unit increase in the tempo means, that if you compare tracks with the lowest tempo to tracks with the average tempo the popularity of a track will increase by XX. You can see, this is a bit complicated description. Concluding, we don’t normalize !\n\nmodel_lm1 &lt;- lm(track_popularity ~energy + tempo+danceability+loudness+liveness+valence+speechiness+instrumentalness+mode+key+acousticness+duration_min, data=features_data)\nsummary(model_lm1)\n\n\nCall:\nlm(formula = track_popularity ~ energy + tempo + danceability + \n    loudness + liveness + valence + speechiness + instrumentalness + \n    mode + key + acousticness + duration_min, data = features_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.186 -4.097 -0.869  3.295 24.188 \n\nCoefficients:\n                 Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)      83.66444    1.88708   44.34 &lt; 0.0000000000000002 ***\nenergy           -3.92141    1.33573   -2.94              0.00338 ** \ntempo             0.00161    0.00540    0.30              0.76581    \ndanceability     -1.47713    1.12573   -1.31              0.18968    \nloudness          0.25623    0.06574    3.90              0.00010 ***\nliveness          1.25557    1.16093    1.08              0.27966    \nvalence          -0.26439    0.71574   -0.37              0.71189    \nspeechiness      -9.03458    1.50650   -6.00         0.0000000026 ***\ninstrumentalness -1.01855    1.01300   -1.01              0.31484    \nmode             -0.27865    0.29903   -0.93              0.35158    \nkey              -0.04467    0.04033   -1.11              0.26814    \nacousticness     -0.84766    0.74340   -1.14              0.25438    \nduration_min     -0.53182    0.15313   -3.47              0.00053 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.39 on 1394 degrees of freedom\nMultiple R-squared:  0.0485,    Adjusted R-squared:  0.0403 \nF-statistic: 5.93 on 12 and 1394 DF,  p-value: 0.000000000381\n\n\n\nseems only that the energy, the loudness, the speechiness and the duration in minutes have significant and substantial effects.\naccording to this linear regression, if the energy of a track is very slow, quiet and calm, a tracks popularity is reduced by 4 points, holding the other factors constant. Consistent to this, if a song is louder by ten decibels, the tracks popularity increases bt 2.5.\nWhat seems also very important is the speechiness of songs. Songs with a high number of spoken words have on average 10 points lower popularity as songs with only a few spoken words.\nthe longest track in the data set is over 9 minutes. If a track is one minute shorter than that, popularity will increase by 0.5 units. However, it would be a bit counterintuitive to assume a linear relationsship here.\n\nSo far, so good. In this model a linear relationsship is assumed for each feature. It is highly possible, that we are extremely underfitting the data and the model is not flexible enough for the patterns in reality.\nLet’s plot the predicted and the actual values of the data set against each other.\n\npop_pred &lt;- predict(model_lm1, newdata=features_data)\npop_pred_data &lt;- data.frame(actual_pop =features_data$track_popularity,\n                            pop_pred = pop_pred)\npred_scatter &lt;- ggplot(pop_pred_data, aes(x=pop_pred, y=actual_pop))+\n  geom_point(alpha=0.5, col=vi.col_1)+\n  labs(title= \"Prediction of the tracks popularity\", x= \"Predicition\", y=\"Given Popularity values\")+\n  theme_own()\npred_scatter\n\n\n\n\n\n\n\n\nWe are far away seeing a linear line here. If model_lm1 would be the perfect model, we could see a linear regression line, because each point on the y axis would be the same as the predicted one."
  },
  {
    "objectID": "index.html#tuning-linear-regressions-and-model-fit",
    "href": "index.html#tuning-linear-regressions-and-model-fit",
    "title": "Exploring polular Spotify Tracks",
    "section": "Tuning Linear Regressions and Model Fit",
    "text": "Tuning Linear Regressions and Model Fit\nWe can assess the model fit using the mse. It gives us insights, how much our model predicitions differ from the observed values.\n\\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y - \\hat{y})^2\n\\]\n\nmse &lt;- function(y_true, y_pred) {\n  mean((y_true - y_pred)^2) \n}\n\n\n#What is the mean squared error for the training set?\nmse(train_data$track_popularity, predict(model_lm1, train_data))\n\n[1] 28.8\n\n\nThe average deviation from our prediction is 28.8, what is quite high against the background that the minimum popularity ios 68 and the highest 100.\nIn the next step, we compute the mse for the validation set. The MSE of our validation set helps us to finde the model, that fits not the data it is generated from, but the another subset of the same the same data to avoid overfitting.\n\nmse(val_data$track_popularity, predict(model_lm1, val_data))\n\n[1] 28.9\n\n\nWe can see, the MSE gets slightly worse, if we predict our model on the validation set.\nUsing all the features, not all of them seems to be substantial and significant. Sorting all the insignificant factors out, could be problmatic, because for example the effect of energy is computed, holding the other features constant. A better option is, to identify the best subset of the factors.\nTherefore, we generate formulas for each possible combination of the features, restricting the number of parameters to 6.\n\ngenerate_formulas &lt;- function(p, x_vars, y_var) {\n  # Input checking\n  if (p %% 1 != 0)           stop(\"Input an integer n\")\n  if (p &gt; length(x_vars))    stop(\"p should be smaller than number of vars\")\n  if (!is.character(x_vars)) stop(\"x_vars should be a character vector\")\n  if (!is.character(y_var))  stop(\"y_vars should be character type\")\n  \n  # combn generates all combinations, apply turns them into formula strings\n  apply(combn(x_vars, p), 2, function(vars) {\n    paste0(y_var, \" ~ \", paste(vars, collapse = \" + \"))\n  })\n}\n\n#get the name of the features\nfeatures_name &lt;- colnames(features_data[2:13]) %&gt;% as.character()\n\nformulas &lt;- generate_formulas(p=6, #number of predictors\n                               x_vars=features_name,\n                               y_var= \"track_popularity\")\n# have a look in the 6 first formulas\nhead(formulas)\n\n[1] \"track_popularity ~ energy + tempo + danceability + liveness + loudness + valence\"         \n[2] \"track_popularity ~ energy + tempo + danceability + liveness + loudness + speechiness\"     \n[3] \"track_popularity ~ energy + tempo + danceability + liveness + loudness + instrumentalness\"\n[4] \"track_popularity ~ energy + tempo + danceability + liveness + loudness + mode\"            \n[5] \"track_popularity ~ energy + tempo + danceability + liveness + loudness + key\"             \n[6] \"track_popularity ~ energy + tempo + danceability + liveness + loudness + acousticness\"    \n\n#how many formulas have we?\nlength_formulas &lt;- length(formulas)\n\nNow, let’s create a function to get the MSE of the linear regessions applied on the validation set.\n\n# create MSE function\nlm_mse &lt;- function(formula, train_data, val_data) {\n  y_name &lt;- as.character(formula)[2]\n  y_true &lt;- val_data[[y_name]]\n  \n  lm_fit &lt;- lm(formula, train_data)\n  y_pred &lt;- predict(lm_fit, newdata = val_data)\n  \n  mean((y_true - y_pred)^2)\n}\n\n#create a vector for MSE values\nmses6 &lt;- rep(0,length_formulas)\n\nfor(i in 1:length_formulas){\n mses6[i] &lt;- lm_mse(as.formula(formulas[i]), train_data, val_data)\n}\n\nsix_pred &lt;-formulas[which.min(mses6)]\nsix_pred\n\n[1] \"track_popularity ~ energy + liveness + loudness + instrumentalness + acousticness + duration_min\"\n\n\nAnd now the same procedure for the five and four best predictors.\n\nformulas &lt;- generate_formulas(p=5, #number of predictors\n                               x_vars=features_name,\n                               y_var= \"track_popularity\")\nlength_formulas &lt;- length(formulas)\n\nmses5 &lt;- rep(0,length_formulas)\n\nfor(i in 1:length_formulas){\n mses5[i] &lt;- lm_mse(as.formula(formulas[i]), train_data, val_data)\n}\n\nfive_pred &lt;-formulas[which.min(mses5)]\nfive_pred\n\n[1] \"track_popularity ~ energy + liveness + loudness + acousticness + duration_min\"\n\n\n\nformulas &lt;- generate_formulas(p=4, #number of predictors\n                               x_vars=features_name,\n                               y_var= \"track_popularity\")\nlength_formulas &lt;- length(formulas)\n\nmses4 &lt;- rep(0,length_formulas)\n\nfor(i in 1:length_formulas){\n mses4[i] &lt;- lm_mse(as.formula(formulas[i]), train_data, val_data)\n}\n\nfour_pred &lt;-formulas[which.min(mses4)]\nfour_pred\n\n[1] \"track_popularity ~ energy + liveness + loudness + duration_min\"\n\n\nSeems like the energy, the loudness, the speechiness and the duration are important factors in all the models.\nAnd which of the models has the smallest MSE on the validation set?\n\n\"MSE of the four predictor model\"\n\n[1] \"MSE of the four predictor model\"\n\nmin(mses4)\n\n[1] 28.8\n\n\"MSE of the five predictor model\"\n\n[1] \"MSE of the five predictor model\"\n\nmin(mses5)\n\n[1] 28.8\n\n\"MSE of the dix predictor model\"\n\n[1] \"MSE of the dix predictor model\"\n\nmin(mses6)\n\n[1] 28.8\n\n\nInteresting! All of the models have the same MSE and it’s quite high! To do this for all possible combinations could be a bit cumbersome and needs a lot of computational power. Intead, we could use a lasso regression, what is a good option, when a realtively small number of predictors have substantial coefficients like in this regression. For more infos about LASSO see this Chapter\n\n#create model matrix\nx_train &lt;- model.matrix(track_popularity ~ ., data=train_data)\nhead(x_train)\n\n  (Intercept) energy tempo danceability liveness loudness valence speechiness\n1           1  0.592   158        0.521   0.1220    -7.78   0.535      0.0304\n2           1  0.507   105        0.747   0.1170   -10.17   0.438      0.0358\n3           1  0.808   109        0.554   0.1590    -4.17   0.372      0.0368\n4           1  0.910   113        0.670   0.3040    -4.07   0.786      0.0634\n5           1  0.783   149        0.777   0.3550    -4.48   0.939      0.2600\n6           1  0.582   117        0.700   0.0881    -5.96   0.785      0.0356\n  instrumentalness mode key acousticness duration_min\n1           0.0000    0   6       0.3080         4.19\n2           0.0608    1   2       0.2000         3.51\n3           0.0000    1   1       0.2140         2.77\n4           0.0000    0   0       0.0939         2.62\n5           0.0000    0   0       0.0283         2.83\n6           0.0000    0  11       0.0502         3.64\n\n\n\n# let's do a lasso regression\nresult &lt;- glmnet(x      = x_train[, -1],          # X matrix without intercept\n                 y      = train_data$track_popularity, \n                 alpha =1 )                      # using k-fold validation to tune lambda\n# plot\nplot(result, xvar='lambda')\n\n\n\n\n\n\n\n\nOverall, we have 12 predictors included.\nThat gives good insights, at which lambda another or multiple predictors are set to zero.\nA better way to use lambda is to use cross-validation.\n\nx_cv &lt;- model.matrix(track_popularity ~ ., bind_rows(train_data, val_data))[, -1]\nresult_cv &lt;- cv.glmnet(x = x_cv, y = c(train_data$track_popularity, val_data$track_popularity), nfolds = 15)\nbest_lambda &lt;- result_cv$lambda.min\nbest_lambda\n\n[1] 0.089\n\n\nWhich coefficients are part of the minimum lambda value?\n\nround(cbind(\n  coef(result_cv, s = 'lambda.min')),3)\n\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)      79.604\nenergy           -1.764\ntempo             .    \ndanceability      .    \nliveness          1.050\nloudness          0.152\nvalence          -0.211\nspeechiness      -7.087\ninstrumentalness -1.102\nmode              .    \nkey              -0.022\nacousticness      .    \nduration_min     -0.397\n\n\nThe best model according to this lasso regression is without the tempo and the mode features.\nLet’s assess the performance of the model.\n\n# Model 1\nmse(test_data$track_popularity, predict(model_lm1, test_data))\n\n[1] 28.7\n\n# best subset of four\npred_sub &lt;- predict(lm(four_pred, data = train_data),newdata = test_data)\nmse(test_data$track_popularity, pred_sub)\n\n[1] 31.1\n\n# cv.lasso\ntest &lt;- model.matrix(track_popularity ~ ., data = test_data)[, -1]\npred_las &lt;- as.numeric(predict(result_cv, newx=test, s=best_lambda))\nmse(test_data$track_popularity, pred_las)\n\n[1] 29.3\n\n\nOverall, we could see the linear regression models have a very bad MSE. We can do better than that ! Most of the relationsships in the data aren’t linear, because it’s more about the sweet spot!"
  },
  {
    "objectID": "index.html#beyond-linearity",
    "href": "index.html#beyond-linearity",
    "title": "Exploring polular Spotify Tracks",
    "section": "Beyond linearity",
    "text": "Beyond linearity\nNow, we investigate the features one by one in order to identify, how they are connected to the popularity.\nFor better visualization, the following function is helpful:\n\npred_plot &lt;- function(model) { #defines the function, needs model as an argument\n  x_pred &lt;- seq(min(train_data$energy), max(train_data$energy), length.out = 986) # set the x value for pred from the lowest lstat value to the highest. We now get a vector with 500 values from the lowest lstat to the highest. The numbers are generated randomly, so we have simulated data.\n  y_pred &lt;- predict(model, newdata = tibble(energy = x_pred)) #we predict the numeric outcome for y based on model we have defined and use for the prediction the randomly generated data x_pred\n  \n  train_data %&gt;% #we set the data that we want to use\n    ggplot(aes(x = energy, y = track_popularity)) + #we define the asthetics and map them\n    geom_point(alpha= 0.7, color= \"steelblue4\") + #we want a scatterplot, so we define the associated geom\n    geom_line(data = tibble(energy = x_pred, track_popularity = y_pred), size = 1, col = \"purple\") + #we add a line for the predicted y on the predicted x data\n    theme_own() # we define a nice theme\n}\n\nA four-degree polynomial.\n\npoly3_mod &lt;- lm(track_popularity ~poly(energy,5), data=train_data)\npred_plot(poly3_mod)\n\n\n\n\n\n\n\n\nFor a model with splines.\n\n#for the knots\nns_mod &lt;- lm(track_popularity ~ bs(energy, df=2, knots=c(0.2,0.5,0.8)), data= train_data)\npred_plot(ns_mod)"
  }
]